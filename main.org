#+TITLE: *Big Data in Health Care*
#+AUTHOR: Federico Moiraghi - 799735 & Pranav Kasela - 846965
#+DATE: A.A. 2019/2020
#+OPTIONS: toc:nil
#+LANGUAGE: it

#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS: [a4paper, 12pt]

* Abstract :ignore:
#+begin_abstract
Obiettivo del presente Progetto è di fornire due modelli predittivi che riescano a riconoscere i tumori omogenei da quelli eterogenei.
Infatti l'eterogeneità del tumore rappresenta una difficoltà aggiuntiva nella fase di trattamento, rendendolo maggiormente resistente alle cure.
Il primo modello, /supervised/, sarà facilmente interpretabile da un esperto di dominio, in modo tale da supportare le sue decisioni senza sostituirsi completamente ad esso.
Tale modello sarà poi confrontato con uno /unsupervised/, che sottolinea le analogie tra i singoli casi.
#+end_abstract

* Indice :ignore:

#+TOC: headlines 1
#+LATEX: \thispagestyle{empty}
#+LATEX: \newpage


* Introduzione
Si è deciso di sviluppare il presente Progetto coi linguaggi di programmazione Python ed R, data la forte crescita del loro uso sia in ambiente accademico che produttivo.
Il primo è usato soprattutto per l'estrazione delle /features/ dalle immagini, grazie alla libreria [[https://github.com/Radiomics/pyradiomics][=pyradiomics=]] che offre numerosi algoritmi, mentre il secondo per l'analisi dei dati, data l'ampia scelta di modelli di /machine learning/.

#+BEGIN_SRC python :session :tangle yes :exports none :results none
import pandas as pd
from radiomics import featureextractor

#nii image reader
import SimpleITK as sitk
import numpy as np

import multiprocessing as mp
import os

#indicating the features required
extract_this = {"shape":      ["Maximum3DDiameter",
                               "MajorAxisLength", "Sphericity",
                               "MinorAxisLength", "SurfaceArea",
                               "SurfaceVolumeRatio",
                               "Flatness", "VoxelVolume"],
                "firstorder": ["Entropy", "Kurtosis", "Maximum",
                               "Mean", "Median", "Minimum",
                               "MeanAbsoluteDeviation",
                               "Skewness", "Variance"],
                "ngtdm":      ["Contrast", "Coarseness"]}

#initialize the featureextractor and define the required features
extractor = featureextractor.RadiomicsFeatureExtractor()
extractor.disableAllFeatures()
extractor.enableFeaturesByName(**extract_this)

features = ["diagnostics_Mask-original_VoxelNum"]
features_name = ["VoxelNum"]
for key in extract_this.keys():
    for elem in extract_this.get(key):
        features.append("original_" + key + "_" + elem)
        features_name.append(elem)

features_name.append("y")

homImagePath = "./code__esempi/lesions/homogeneous/nifti/"
homImages = [(homImagePath+file, 0) for file in os.listdir(homImagePath)]

hetImagePath = "./code__esempi/lesions/heterogeneous/nifti/"
hetImages = [(hetImagePath+file, 1) for file in os.listdir(hetImagePath)]

images = homImages + hetImages

def get_feature_df(path):
    img    = sitk.ReadImage(path[0])
    mask   = img > 0
    infos  = extractor.execute(img, mask)
    result = [float(infos[f]) for f in features]
    result.append(path[1])
    return result

#some parallelization
pool = mp.Pool(3)
res = pool.map(get_feature_df, images)

#the final df
final_df = pd.DataFrame(res, columns=features_name)

final_df.to_csv("feature_dataset.csv", index=None)
#+END_SRC

Caricate le immagini (si ha un esempio con figura [[section_example]]), si nota che queste rappresentano la lesione già segmentata.
Non si ritiene dunque necessaria alcuna forma particolare di /pre-processing/ sull'immagine.

#+BEGIN_SRC python :session :exports results :results file graphics :file images/sample.png
import matplotlib.pyplot as plt


x_1 = sitk.ReadImage(hetImages[14][0])
x = sitk.GetArrayFromImage(x_1)

fig = plt.figure()
count = 1
for z in range(x.shape[2]):
    if z > 4 and z < 14:
        plt.subplot(3, 3, count)
        plt.imshow(x[:, :, z], cmap="gist_heat")
        plt.axis("off")
        count += 1
#+END_SRC

#+LABEL: section_example
#+CAPTION: Esempio di immagine. Essendo una figura tridimensionale, si rappresenta la profondità con più immagini.
#+RESULTS:
[[file:images/sample.png]]


Mentre le dimensioni dei /voxel/ sono fisse per tutti i file, l'immagine è ritagliata sulla lesione in modo specifico, tralasciando le parti adiacenti: le dimensioni variano in base alla dimensione della lesione stessa.

#+BEGIN_SRC python :session :exports results :results dataframe :rownames yes :colnames no
dim_x = x_1.GetMetaData("pixdim[1]")
dim_y = x_1.GetMetaData("pixdim[1]")
dim_z = x_1.GetMetaData("pixdim[1]")

res = pd.DataFrame({"x":[round(float(dim_x), 3)],
                    "y":[round(float(dim_y), 3)],
                    "z":[round(float(dim_z), 3)]},
                   index = ["voxel size"])

res.T
#+END_SRC

#+RESULTS:
:    voxel size
: x       2.734
: y       2.734
: z       2.734

Dunque sarà importante estrarre delle /features/ che non dipendano dalla dimensione dell'immagine ma tengano conto di possibili variazioni.
Questo approccio comporta una serie di vantaggi, primo tra tutti la modularità del /workflow/: è possibile così prevedere la variabile risposta avendo a disposizione sia un'immagine già segmentata sia effettuando la segmentazione /on-the-fly/ tramite semplici algoritmi a soglia, riducendo potenzialmente il tempo della diagnosi.

* Estrazione delle /features/
L'estrazione delle /features/ mappa le immagini in uno spazio di dimensionalità molto minore e di dimensioni fisse, rendendo più semplice l'analisi dato il numero esiguo di dati a disposizione.
Infatti, un qualsiasi algoritmo di /machine learning/ ha bisogno di un numero significativo di dati per  ``apprendere'' in modo /data-driven/ cosa utilizzare nell'analisi.

Per selezionare le /features/ da utilizzare, si è preso spunto da cite:imaging[fn::Gli autori usano i primi quattro momenti per stimare la differenza di eterogeneità di tumori alla cervicale nel tempo, a seguito di un trattamento.] e cite:gallivanone18_param_influen_pet_imagin_featur: si usano /features/ estratte direttamente dall'immagine, quali superficie della lesione o la sua sfericità, accompagnate da indici statistici più semplici quali i momenti di ordine dal primo fino al quarto (media, varianza, asimmetria e curtosi) dei valori dei singoli /voxel/ (tabella [[tbl_features]]).

#+BEGIN_SRC R :session :tangle yes :exports none :results none
rm(list = ls())
set.seed(20200623)
#+END_SRC

#+BEGIN_SRC R :session :tangle yes :exports none :results none
library(tidyverse)

features <- readr::read_csv("./feature_dataset.csv")
features <- features %>%
  mutate_at(setdiff(colnames(features),
                    c("y")),
            ~(scale(.) %>% as.vector))
features <- features[sample(nrow(features)), ]
#+END_SRC

#+BEGIN_SRC R :session :exports results :results table :rownames no :colnames yes
round(head(features[, 1:4]), 3)
#+END_SRC

#+LABEL: tbl_features
#+CAPTION: Esempio di /features/ estratte per le singole immagini.
#+RESULTS:
| VoxelNum | Maximum3DDiameter | MajorAxisLength | Sphericity |
|----------+-------------------+-----------------+------------|
|    0.485 |             0.356 |           0.028 |      0.787 |
|   -0.295 |            -0.719 |          -0.747 |       0.85 |
|    0.458 |             0.118 |          -0.106 |      0.956 |
|   -0.341 |              -0.6 |          -0.021 |     -1.781 |
|   -0.326 |            -0.719 |          -0.751 |      0.736 |
|   -0.739 |             -0.14 |           0.062 |     -1.256 |


Tuttavia il numero di /features/ estratte è ancora elevato rispetto al numero di dati a disposizione (la matrice di /input/ ha dimensioni $44 \times 20$).
Si effettua dunque una prima selezione delle /features/ tramite il test di Mann-Whitney, equivalente non parametrico del t-test (i risultati sono riassunti nella tabella [[tbl_mann_whitney]]), per escludere le variabili la cui significatività, prese singolarmente, è minore del 5%.

#+BEGIN_SRC R :session :tangle yes :exports none :results none
score <- c()
for (i in seq(1, dim(features)[2] - 1)) {
  formula <- paste0(colnames(features)[i], " ~ y")
  t_score <- wilcox.test(formula = as.formula(formula),
                         data = features)$p.value
  score <- c(score, round(t_score, 3))
}
score_df <- data.frame(t(score))
colnames(score_df) <- colnames(features)[1:(dim(features)[2] - 1)]
accepted <- colnames(score_df[, score_df < 0.05])

features <- features[, c(accepted, "y")]
#+END_SRC

#+BEGIN_SRC R :session :exports results :results table :rownames yes
t(score_df[order(score_df)])
#+END_SRC

#+LABEL: tbl_mann_whitney
#+CAPTION: p-value delle varibili dal test di Mann-Whitney.
#+RESULTS:
| Kurtosis              | 0.002 |
| Sphericity            | 0.003 |
| MinorAxisLength       | 0.003 |
| Maximum               | 0.008 |
| Maximum3DDiameter     | 0.011 |
| MajorAxisLength       | 0.012 |
| SurfaceArea           | 0.016 |
| Variance              | 0.025 |
| Coarseness            | 0.034 |
| Skewness              |  0.04 |
| MeanAbsoluteDeviation | 0.051 |
| Mean                  | 0.079 |
| Entropy               | 0.102 |
| Median                | 0.137 |
| Contrast              | 0.157 |
| Minimum               | 0.204 |
| VoxelNum              | 0.219 |
| VoxelVolume           | 0.219 |
| Flatness              | 0.291 |
| SurfaceVolumeRatio    | 0.878 |

Effettuata questa prima cernita, si riduce ulteriormente il numero di /features/, in modo tale da evitare multi-collinearità tra le variabili, rispettando così le premesse del modello lineare.

#+BEGIN_SRC R :session :exports results :results file graphics :file images/corrplot.png
library(ggcorrplot)


ggcorrplot::ggcorrplot(
              cor(features),
              type = "lower",
              outline.col = "white",
              lab = TRUE)
#+END_SRC

#+LABEL: features_corr
#+CAPTION: Correlogramma delle /features/ estratte.
#+RESULTS:
[[file:images/corrplot.png]]

Dal correlogramma (figura [[features_corr]]) si deduce quali variabili escludere (=Maximum=, =Variance=, =Maximum3DDiameter=, =MinorAxisLength=, =Contrast= e =Sphericity=): la matrice risultante ha una dimensionalità ridotta ($44 \times 5$), adeguata per la costruzione del modello.

#+BEGIN_SRC R :session :tangle yes :exports none :results none
new_cols <- setdiff(colnames(features),
                    c("Maximum", "Variance",
                      "Maximum3DDiameter",
                      "MinorAxisLength",
                      "Contrast", "Sphericity"))

features <- features[, new_cols]
#+END_SRC

Nella figura [[fig:eda]], viene mostrata la distribuzione di densità delle variabili accetate condizionati alla tipologia di lesioni, in modo da vedere graficamente la differenza nella loro distribuzione.

#+BEGIN_SRC R :session :exports results :results file graphics :file images/plot1.png
library(ggridges)
library(ggthemes)
library(gridExtra)
library(grid)
tema <- theme(plot.title=element_text(size=12, hjust=.5,
                                      vjust=0, color="black"),
        axis.title.y=element_text(size=12, vjust=2, color="black"),
        axis.title.x=element_text(size=14, vjust=-1, color="black"),
        axis.text.x=element_text(size=12, color="black"),
        axis.text.y=element_text(size=12, color="black"),
        legend.position="None")

features$Hom <- ifelse(features$y==1, "Het.", "Hom.")

HisMAL <- ggplot(data=features,
                 aes(x=MajorAxisLength,  y=Hom))+
  geom_density_ridges(alpha=.4, aes(fill=y)) +
  xlab("MajorAxisLength") +
  ylab("Density") +
  theme(legend.position="none")+
                  theme_economist() +
                  theme_solarized(light=T)+
                  scale_colour_solarized('blue') + tema

HisSA <- ggplot(data=features,
                 aes(x=SurfaceArea,  y=Hom))+
  geom_density_ridges(alpha=.4, aes(fill=y)) +
  xlab("SurfaceArea") +
  ylab("Density") +
  theme(legend.position="none")+
                  theme_economist() +
                  theme_solarized(light=T)+
                  scale_colour_solarized('blue') + tema

HisK <- ggplot(data=features,
                 aes(x=Kurtosis,  y=Hom))+
  geom_density_ridges(alpha=.4, aes(fill=y)) +
  xlab("Kurtosis") +
  ylab("Density") +
  theme(legend.position="none")+
                  theme_economist() +
                  theme_solarized(light=T)+
                  scale_colour_solarized('blue') + tema

HisS <- ggplot(data=features,
                 aes(x=Skewness,  y=Hom))+
  geom_density_ridges(alpha=.4, aes(fill=y)) +
  xlab("Skewness") +
  ylab("Density") +
  theme(legend.position="none")+
                  theme_economist() +
                  theme_solarized(light=T)+
                  scale_colour_solarized('blue') + tema

HisC <- ggplot(data=features,
                 aes(x=Coarseness,  y=Hom))+
  geom_density_ridges(alpha=.4, aes(fill=y)) +
  xlab("Coarseness") +
  ylab("Density") +
  theme(legend.position="none")+
                  theme_economist() +
                  theme_solarized(light=T)+
                  scale_colour_solarized('blue') + tema

features %>% dplyr::select(-Hom) -> features

grid.arrange(HisMAL + ggtitle(""),
             HisSA + ggtitle(""),
             HisK + ggtitle(""),
             HisS  + ggtitle(""),
             HisC  + ggtitle(""),
             layout_matrix = matrix(c(1,3,2,5,4,2),
                                    nrow=3),
             top = textGrob("Density Plots",
                            gp=gpar(fontsize=15)))
#+END_SRC

#+LABEL: fig:eda
#+CAPTION: Density plot delle variabili scelte per tipo di lesioni.
#+RESULTS:
[[file:images/plot1.png]]

* Modello /supervised/
Essendo la variabile risposta binaria (tumore /omogeneo/ o /eterogeneo/, rispettivamente 0 o 1), e volendo costruire un modello facilmente interpretabile per un esperto di dominio, si effettua una semplice regressione logistica.

#+BEGIN_SRC R :session :tangle yes :exports none :results none
library(MASS)


formula <- stepAIC(glm(y ~  MajorAxisLength + SurfaceArea + Kurtosis +
                            Skewness + Coarseness,
                       data = features,
                       family = binomial("logit")),
                   direction = "both",
                   k = log(nrow(features)))$formula
mod_full <- glm(formula, data = features, family = binomial("logit"))
#+END_SRC

La selezione delle /features/ è effettuata tramite procedimento /stepwise/ usando l'indice BIC[fn::L'indice BIC rispetto all'indice AIC penalizza maggiormente l'inserimento di una nuova variabile con un numero ridotto di osservazioni.], con possibilità di re-immissione.
Il numero di variabili significative si riduce quindi a tre: =SurfaceArea=, =Kurtosis= e =Skewness= (riassunti nella tabella [[tbl_model_coeff]] coi rispettivi p-value).

#+BEGIN_SRC R :session :exports results :results tabular :colnames yes :rownames yes
df <- summary(mod_full)$coefficients[, c(1, 4)]
colnames(df) <- c("Stima", "p-value")
round(df, 6)
#+END_SRC

#+LABEL: tbl_model_coeff
#+CAPTION: Stima dei coefficienti del modello e loro significatività.
#+RESULTS:
|             |     Stima |  p-value |
|-------------+-----------+----------|
| (Intercept) |  4.295873 | 0.014587 |
| SurfaceArea | 11.899879 | 0.005449 |
| Kurtosis    |  9.842963 | 0.008876 |
| Skewness    |  8.655905 | 0.007367 |

Il modello, come ci si aspettava, sottolinea la correlazione tra la superficie della lesione e la sua eterogeneità: un tumore eterogeneo, infatti, ha spesso una forma irregolare e dunque una superficie maggiore.
Inoltre si nota come anche curtosi e asimmetria positiva siano relazionate con la probabilità di eterogeneità: se un tumore è composto da componenti più ``ghiotte'' (e dunque aggressive), i rispettivi /voxel/ risultano maggiormente visibili e quindi entrambi gli indici aumentano.

#+BEGIN_SRC R :session :tangle yes :exports none :results none
accuracy <- function(y_true, y_hat) {
  return(mean(y_true == y_hat))
}

precision <- function(y_true, y_hat) {
  tp <- mean(y_hat == 1 & y_true == 1)
  fp <- mean(y_hat == 1 & y_true == 0)
  return(tp / (tp + fp))
}

recall <- function(y_true, y_hat) {
  tp <- mean(y_hat == 1 & y_true == 1)
  fn <- mean(y_hat == 0 & y_true == 1)
  if (fn == 0) return(1)
  return(tp / (tp + fn))
}

f1 <- function(y_true, y_hat) {
  p <- precision(y_true, y_hat)
  r <- recall(y_true, y_hat)
  return(2 * p * r / (p + r))
}

features$y <- as.factor(features$y)
k <- 30
dim_fold <- 9
out <- list(accuracy = c(),
            precision = c(),
            recall = c(),
            f_1 = c())

for (i in seq(1, k)) {
  set.seed(i)
  test_index <- sample(seq(1, dim(features)[1]), dim_fold)
  train_set <- features[-test_index, ]
  test_set  <- features[ test_index, ]

  mod <- glm(formula,
             data = train_set,
             family = binomial("logit"))

  y_hat <- ifelse(predict(mod, test_set) > 0.5, 1, 0)
  y_true <- test_set$y
  out$accuracy  <- c(out$accuracy,  accuracy(y_true, y_hat))
  out$precision <- c(out$precision, precision(y_true, y_hat))
  out$recall    <- c(out$recall,    recall(y_true, y_hat))
  out$f_1       <- c(out$f_1,       f1(y_true, y_hat))
}
#+END_SRC

Le prestazioni del modello sono calcolate col sistema /iterated holdout/, effettuando 30 iterazioni casuali dividendo i dati 80% /train set/ e 20% /test set/, così da avere stime robuste dei parametri e un intervallo di confidenza sufficientemente ristretto.
La media degli indici di bontà è riportata nella tabella [[supervised_results]] assieme al rispettivo intervallo di confidenza al 99%.

#+BEGIN_SRC R :session :exports results :results table :rownames yes :colnames yes
out_df <- data.frame(index = c("accuracy", "precision", "recall", "f_1"))
scores <- c()
idc <- c()
for (index in out_df$index) {
  score <- out[[index]]
  score <- score[!is.nan(score)]
  mu <- mean(score)
  s  <- sd(score)
  d <- qt(0.99, length(score) - 1) * s / sqrt(length(score))
  scores <- c(scores, mu)
  idc <- c(idc, d)
}
out_df$average <- scores
out_df$IDC_99   <- idc
rownames(out_df) <- out_df$index
round(out_df[, c("average", "IDC_99")], 3)
#+END_SRC

#+LABEL: supervised_results
#+CAPTION: Performance del modello supervisionato con intervallo di confidenza al 99%.
#+RESULTS:
|           | average | IDC_99 |
|-----------+---------+--------|
| accuracy  |   0.878 |  0.038 |
| precision |   0.838 |  0.085 |
| recall    |   0.857 |  0.083 |
| f_1       |   0.822 |  0.066 |

Si calcola quindi anche la matrice di confusione (tabella [[supervised_confusion_matrix]]), e si nota che sia molto performante in questo caso.

#+BEGIN_SRC R :session :exports results :results tabular :colnames yes :rownames yes
previsions <- data.frame(prevision = ifelse(predict(mod, test_set) > 0.5,
                                            "prediction: heterogeneous",
                                            "prediction: homogeneous"),
                         real = ifelse(test_set$y == 1,
                                       "heterogeneous",
                                       "homogeneous"))
conf_matrix <- table(previsions)
#+END_SRC

#+LABEL: supervised_confusion_matrix
#+CAPTION: Matrice di confusione del modello di regressione logistica per il /test set/; sulle righe le previsioni e sulle colonne i valori reali.
#+RESULTS:
|                           | heterogeneous | homogeneous |
|---------------------------+---------------+-------------|
| prediction: heterogeneous |             3 |           0 |
| prediction: homogeneous   |             2 |           4 |

* Modello /unsupervised/
Considerando tutti i dati (quindi più informazione possibile), standardizzati, si effettua una divisione in /clusters/ con l'ipotesi che sia possibile raggruppare le due tipologie di tumore.

#+BEGIN_SRC R :session :tangle yes :exports none :results none
features <- readr::read_csv("./feature_dataset.csv")
features <- features %>%
  mutate_at(setdiff(colnames(features),
                    c("y")),
            ~(scale(.) %>% as.vector))
features$y <- as.factor(features$y)

features <- features[, c(accepted, "y")]
#+END_SRC
Vengono rimosse dal dataset, tutte le features che non avevano superato il test di Mann-Whitney, infatti, possono essere viste come variabili che potenzialmente introdurrano rumore nel clustering.

Le immagini di tumori quindi sono collocate in uno spazio vettoriale in base al risultato della /Principal Component Analysis/ (PCA): si selezionano così le prime 4 componenti, che spiegano almeno il 5% della varianza della distribuzione.
Così, oltre a operare su una matrice di dimensioni ridotte, si riduce anche la quantità di rumore data dall'elevato numero di variabili (a cui si esclude la variabile risposta =y=, usata poi per calcolare la bontà del modello) spesso inutili.
Dalla figura [[fig:pca_plot]] infatti si vede che all'aumentare del numero di componenti considerate, la percentuale di varianza spiegata dalla componente decresce: la soglia del 5% è un compromesso tra il segnale colto dal modello e la sua complessità (per i dettagli vedere la tabella [[table_pca]]). Le 4 componenti scelte spiegano complessivamente circa il 90% della varianza totale.


#+BEGIN_SRC R :session :exports results :results graphics file :file images/pca_unsupervised.png
eigen_values <- eigen(var(features %>% dplyr::select(-y)))$values
perc_variance <- eigen_values / sum(eigen_values)

data.frame(number_of_components = seq(1, dim(features)[2] - 1),
           variance=perc_variance) %>%
  ggplot(aes(x = number_of_components, y = variance)) +
  geom_bar(stat = "identity") + geom_point(size = 3) +
  geom_line() +
  # geom_hline(aes(yintercept = 1), alpha = 0.3) +
  geom_hline(aes(yintercept = 0.05), color = "red",
             alpha = 0.4) +
  xlab("Number of Components") + ylab("% variance") +
  ggtitle("Selection of number of components of PCA") +
  theme_minimal()
#+END_SRC

#+LABEL: fig:pca_plot
#+CAPTION: Andamento della varianza spiegata dal modello all'aumentare del numero di componenti della PCA.
#+RESULTS:
[[file:images/pca_unsupervised.png]]

#+BEGIN_SRC R :session :tangle yes :exports none :results none
unsupervised_features <- features[, 1:(dim(features)[2] - 1)]
data.pca <- prcomp(unsupervised_features)
#+END_SRC

#+BEGIN_SRC R :session :exports results :results table :colnames yes :rownames yes
round(summary(data.pca)$importance[, 1:5], 3)
#+END_SRC

#+LABEL: table_pca
#+CAPTION: Alcune statistiche sulle prime componenti principali.
#+RESULTS:
|                        |   PC1 |   PC2 |   PC3 |   PC4 |   PC5 |
|------------------------+-------+-------+-------+-------+-------|
| Standard deviation     | 2.097 | 1.595 | 1.406 | 0.967 | 0.689 |
| Proportion of Variance |   0.4 | 0.231 |  0.18 | 0.085 | 0.043 |
| Cumulative Proportion  |   0.4 | 0.631 | 0.811 | 0.896 | 0.939 |


Nello spazio della PCA si effettua un raggruppamento usando l'algoritmo DBScan, basato sulla densità delle osservazioni.
La figura [[fig:DBScan_eps]] suggerisce un parametro $\varepsilon \in (1.8, 2)$ (con 5-NN), si opta per il valore medio: questa configurazione sarà usata per la costruzione del modello.


# Si effettua un primo tipo di clustering basato sulla densita' (nello spazio dei PCA) usando uno dei modelli piu' usati per il clustering, il DBScan. Nella Figura [[fig:DBScan_eps]] notiamo che eps piu' vantaggioso usando la distanza di 5-NN risulta vicino al 3.5, quindi viene scelto esso come il valore ideale, per il clustering.

#+BEGIN_SRC R :session :exports results :results graphics file :file images/dbscan_eps_selection.png
data <- data.pca$x[, 1:4]

dbscan::kNNdistplot(data, k = 5)
abline(h = 1.8, lty = 2, col = "red")
abline(h = 2, lty = 2, col = "blue")
#+END_SRC

#+LABEL: fig:DBScan_eps
#+CAPTION: Scelta del valore $\varepsilon$ per DBScan.
#+RESULTS:
[[file:images/dbscan_eps_selection.png]]

#+BEGIN_SRC R :session :tangle yes :exports none :results none
data <- data.pca$x[,1:4]

cluster <- dbscan::dbscan(data, 1.9)
#+END_SRC

#+BEGIN_SRC R :session :exports results :results table :rownames yes
out <- data.frame("accuracy"  = accuracy(1 - cluster$cluster, features$y),
                  "precision" = precision(1 - cluster$cluster, features$y),
                  "recall"    = recall(1 - cluster$cluster, features$y),
                  "f_1"       = f1(1 - cluster$cluster, features$y))
round(t(out), 3)
#+END_SRC

#+LABEL: dbscan_performance
#+CAPTION: Indici di bontà per la clusterizzazione con DBScan.
#+RESULTS:
| accuracy  | 0.795 |
| precision | 0.556 |
| recall    | 0.909 |
| f_1       |  0.69 |


#+BEGIN_SRC R :session :exports results :results table :rownames yes :colnames yes
HomOrHet <- ifelse(features$y==0, "homogeneous", "heterogeneous")
clus <- paste0("C_", cluster$cluster)
table(clus, HomOrHet)
#+END_SRC

#+LABEL: dbscan_confusion_matrix
#+CAPTION: Distribuzione delle immagini all'interno dei /clusters/.
#+RESULTS:
|     | heterogeneous | homogeneous |
|-----+---------------+-------------|
| C_0 |            10 |           1 |
| C_1 |             8 |          25 |

Nonostante le buone /performance/ del modello (riassunte nella tabella [[dbscan_performance]]), si nota che il secondo /cluster/ $C_1$ contiene un numero non indifferente ma basso di immagini eterogenee.
L'algoritmo è riuscito a individuare un /cluster/ ($C_0$, il cluster di elementi rigettati) ben definito, considerando la variabile risposta.

Si tenta un altro approccio, con l'algoritmo /HK-means/, versione gerarchica del ben più noto /K-means/.
L'algoritmo è quindi testato con un numero di /cluster/ $k$ da 2 a 15, calcolando per ciascuno la distanza nei gruppi (/distance between/).
La figura [[fig:kmean_k]] mostra graficamente il procedimento: si sceglie $k = 4$ per evitare /overfitting/ dei dati, e siccome il tasso di aumento per $k > 4$ decresce fortemente.
La bontà del raggruppamento invece (intesa come capacità predittiva) è riassunta nella tabella
[[hkmeans_performance]].

# Pur avendo un'accuratezza non bassa, si vede che il secondo cluster contiene un numero di immagini omogenee non trascurabili, inoltre il DBScan e' riuscito a trovare solo un cluster, il /CLUSTER_0/ e' un cluster costituito da prova considerate non appartenenti a nessun cluster.
# Quindi si tenta un altro approccio basato su un ibrido tra il clustering gerarchico e il k-means.

#+BEGIN_SRC R :session :tangle yes :results none :exports none
ncluster_score <- c()
for (num_clus in seq(2, 15)){
  cluster <- factoextra::hkmeans(data, num_clus)
  # Calculate silhuoette based on the mode of the cluster.
  ncluster_score <- c(ncluster_score,
                      cluster$betweenss)
}
#+END_SRC


# In questo caso pero' bisogna cercare il numero di cluster ideale, e per fare questo effettiamo il cluster per ciascun k da 2 da 15 e plottiamo le loro misure betweenss e scegliamo un k in base al plot.
# Nella Figura [[fig:kmean_k]] scegliamo il k=4 per non ``overfittare'' il cluster.

#+BEGIN_SRC R :session :exports results :results graphics file :file images/cluster_selection.png
data.frame(number_of_cluster = seq(2, 15),
           sil = ncluster_score) %>%
  ggplot(aes(x = number_of_cluster, y = ncluster_score)) +
  geom_line() + geom_point() +
  geom_vline(aes(xintercept = 4), color="red",
             alpha=0.4) +
  xlab("Number of Clusters K") + ylab("Distance Between") +
  ggtitle("Selection of number of cluster") +
  theme_minimal()
#+END_SRC

#+LABEL: fig:kmean_k
#+CAPTION: Variazione della distanza /between/ all'aumentare del parametro $k$.
#+RESULTS:
[[file:images/cluster_selection.png]]


# Nella tabella seguente si mostrano le misure ottentute dal cluster.

#+BEGIN_SRC R :session :exports results :results table :rownames yes
cluster <- factoextra::hkmeans(data, 4, iter.max = 50)
tp <- 0
tn <- 0
fp <- 0
fn <- 0
for (i in seq(1,4)){
  index <- which(cluster$cluster == i)
  in_clus <- features$y[index]
  homs <- as.numeric(table(in_clus)["0"])
  hets <- as.numeric(table(in_clus)["1"])
  if (homs > hets){
    tn <- tn + homs
    fp <- fp + hets
  } else {
    tp <- tp + hets
    fn <- fn + homs
  }
}
acc <- (tp + tn) / (tp + fn + fp + tn)
rec <- tp / (tp + fn)
prec <- tp / (tp + fp)
f_1 <- 2 * rec * prec / (rec + prec)

round(data.frame(c(acc, prec, rec, f_1),
           row.names=c("accuracy", "precision",
                       "recall", "f_1")),3)
#+END_SRC

#+LABEL: hkmeans_performance
#+CAPTION: Indici di bontà per HK-Means con $k = 4$.
#+RESULTS:
| accuracy  | 0.818 |
| precision | 0.556 |
| recall    |     1 |
| f_1       | 0.714 |

#+BEGIN_SRC R :session :exports results :results table :rownames yes :colnames yes
HomOrHet <- ifelse(features$y == 0, "homogeneous", "heterogeneous")
clus <- paste0("C_", cluster$cluster)
table(clus, HomOrHet)
#+END_SRC

#+LABEL: hkmeans_clusters
#+CAPTION: Il modello ha identificato tre /cluster/ definiti (C_1, C_3 e C_4), considerando la variabile risposta;
#+RESULTS:
|     | heterogeneous | homogeneous |
|-----+---------------+-------------|
| C_1 |             4 |          18 |
| C_2 |             4 |           8 |
| C_3 |             2 |           0 |
| C_4 |             8 |           0 |

# Anche il questo caso il cluster ottiene performance decenti, ma il primo modello riusciva a distinguire le due immagini in una maniera piu' decente e con meno cluster.

Il KKmeans è riuscito ad individuare 3 cluster, 2 per le prove eterogenee e 1 per le prove omogenee ben distinti, mentre /cluster 2/ puo' essere visto come cluster contente prove ambigue in questo caso.

La non separazione totale delle due tipologie di lesioni è dovuta anche alla similarità tra distribuzione delle variabili condizionati alla lesione (figura [[fig:eda]]).

Nonostante, il miglioramento di HK-Means rispetto al modello DBScan, la capacità predittiva dei modelli /unsupervised/, paragonati a quello /supervised/ presentato precedentemente, è nettamente inferiore.
Inoltre, lavorando nello spazio delle componenti principali, l'interpretabilità del modello risulta difficile anche per un esperto di dominio.
#+LATEX: \newpage

* Conclusioni
Con questo Progetto si è costruito un modello statistico /supervised/ efficace e facilmente interpretabile da un esperto di dominio per prevedere l'eterogeneità del tumore.
Per costruirlo è stato sufficiente estrapolare dalle immagini segmentate delle semplici /features/, veloci da calcolare e facili da interpretare.
Si è quindi confrontato questo modello con uno /unsupervised/, confermando la superiorità del primo sia per bontà di previsione sia per facilità di interpretazione.

Per migliorare il modello si potrebbe, a livello teorico, usare un numero maggiore di dati per la stima dei parametri e per selezionare le /features/ da includere; tuttavia questo non è sempre possibile in ambito medico, data la forte difficoltà e l'alto costo nell'ottenere una più grande quantità di dati.
Inoltre, con un numero maggiore di dati, è possibile utilizzare modelli più complessi che considerino anche interazioni tra le variabili, senza rischiare di perdere capacità di generalizzazione.

#+BEGIN_SRC R :session :tangle yes :exports none :results none
summary(mod_full)
#+END_SRC


* Bibliografia :ignore:
#+LATEX: \newpage
#+LATEX: \nocite{*}
bibliographystyle:unsrt
bibliography:./bibliografia.bib
#+BEGIN_SRC bibtex :tangle bibliografia.bib :exports none
@article{imaging,
  author = {Bowen, Stephen and
            Yuh, William and
            Hippe, Daniel and
            Wu, Wei and
            Partridge, Savannah and
            Elias, Saba and
            Jia, Guang and
            Huang, Zhibin and
            Sandison, George and
            Nelson, Dennis and
            Knopp, Michael and
            Lo, Simon and
            Kinahan, Paul and
            Mayr, Nina},
  year = {2017},
  month = {10},
  pages = {},
  title = {Tumor radiomic heterogeneity: Multiparametric functional imaging to characterize variability and predict response following cervical cancer radiation therapy},
  volume = {47},
  journal = {Journal of Magnetic Resonance Imaging},
  doi = {10.1002/jmri.25874}
}

@article{gallivanone18_param_influen_pet_imagin_featur,
  author          = {Francesca Gallivanone and
                     Matteo Interlenghi and
                     Daniela D'Ambrosio and
                     Giuseppe Trifirò and
                     Isabella Castiglioni},
  title           = {Parameters Influencing Pet Imaging Features: a Phantom Study With Irregular and Heterogeneous Synthetic Lesions},
  journal         = {Contrast Media \& Molecular Imaging},
  volume          = {2018},
  number          = {},
  pages           = {1-12},
  year            = {2018},
  doi             = {10.1155/2018/5324517},
  url             = {https://doi.org/10.1155/2018/5324517},
  DATE_ADDED      = {Thu Jun 11 16:47:03 2020},
}
#+END_SRC
#+begin_comment
Local variables:
org-latex-caption-above: nil
eval: (pyvenv-activate (concat (getenv "HOME") "/.anaconda/envs/medical"))
eval: (ispell-change-dictionary "italiano")
End:
#+end_comment

* Anaconda Environment :noexport:
Run =conda env create --file anaconda_environment.yml= to create the environment.
#+BEGIN_SRC yaml :tangle anaconda_environment.yml
name: medical
dependencies:
- python=3.7
- pandas=1.0.4
- numpy=1.18.5
- matplotlib=3.2.1
- simpleitk=1.2.4
- pywavelets=1.0.0
- r-base=4.0.0
- r-mass=7.3_51.6
- r-tidyverse=1.3.0
- r-ggcorrplot=0.1.3
- r-dbscan=1.1_5
- r-factoextra=1.0.7
- r-ggridges=0.5.2
- r-ggthemes=4.2.0
- r-gridextra=2.3
- pip:
  - pyradiomics==3.0
#+END_SRC

#+BEGIN_SRC bash :tangle execute-all.sh
echo "Extracting features..."
python main.py
echo "Training model..."
R -f main.R
#+END_SRC
